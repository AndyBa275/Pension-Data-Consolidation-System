{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ca18df-1eaf-4897-8b47-5096d2cad408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = r\"C:\\Users\\spt-admin\\Desktop\\NEWD\\schedule_records.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Step 1: Clean SSNIT numbers\n",
    "def clean_ssnit(ssnit):\n",
    "    if pd.isna(ssnit):\n",
    "        return ''\n",
    "    # Convert to string, strip whitespace, remove special characters\n",
    "    ssnit = str(ssnit).strip()\n",
    "    ssnit = re.sub(r'[^\\w]', '', ssnit)  # Remove non-alphanumeric\n",
    "    return ssnit.upper()\n",
    "\n",
    "df['CLEANED_SSNIT'] = df['SSNIT_NUMBER'].apply(clean_ssnit)\n",
    "\n",
    "# Check if SSNIT is invalid (empty, 0, unknown, nan, etc.)\n",
    "def is_invalid_ssnit(ssnit):\n",
    "    if pd.isna(ssnit) or ssnit == '':\n",
    "        return True\n",
    "    ssnit_str = str(ssnit).strip().upper()\n",
    "    \n",
    "    # Exact matches for invalid values\n",
    "    invalid_values = ['', '0', 'UNKNOWN', 'NAN', 'NA', 'N/A']\n",
    "    if ssnit_str in invalid_values:\n",
    "        return True\n",
    "    \n",
    "    # Check if it contains certain keywords (TRUST, UNKNOWN, NO_SSF_NUMBER)\n",
    "    invalid_keywords = ['TRUST', 'UNKNOWN', 'NO_SSF_NUMBER']\n",
    "    for keyword in invalid_keywords:\n",
    "        if keyword in ssnit_str:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "df['INVALID_SSNIT'] = df['CLEANED_SSNIT'].apply(is_invalid_ssnit)\n",
    "\n",
    "print(f\"\\nUnique original SSNIT: {df['SSNIT_NUMBER'].nunique()}\")\n",
    "print(f\"Unique cleaned SSNIT: {df['CLEANED_SSNIT'].nunique()}\")\n",
    "print(f\"Records with invalid SSNIT: {df['INVALID_SSNIT'].sum()}\")\n",
    "\n",
    "# Step 2: Parse and clean names\n",
    "def parse_name(name):\n",
    "    if pd.isna(name):\n",
    "        return []\n",
    "    \n",
    "    # Convert to string and clean\n",
    "    name = str(name).strip()\n",
    "    \n",
    "    # Remove quotes and extra spaces\n",
    "    name = name.replace(\"'\", \"\").replace('\"', '')\n",
    "    \n",
    "    # Replace commas with spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    \n",
    "    # Split by whitespace and filter empty strings\n",
    "    parts = [p.strip().upper() for p in name.split() if p.strip()]\n",
    "    \n",
    "    return parts\n",
    "\n",
    "df['NAME_PARTS'] = df['FULL_NAME'].apply(parse_name)\n",
    "\n",
    "# Step 3: Group by cleaned SSNIT and find matches (ONLY for valid SSNITs)\n",
    "def find_name_matches(group):\n",
    "    \"\"\"\n",
    "    Within a SSNIT group, find which records share at least one name component\n",
    "    using fuzzy matching\n",
    "    \"\"\"\n",
    "    if len(group) == 1:\n",
    "        # Only one record, no need to match\n",
    "        return [0]\n",
    "    \n",
    "    n = len(group)\n",
    "    # Create clusters - records that should be grouped together\n",
    "    clusters = []\n",
    "    assigned = [False] * n\n",
    "    \n",
    "    for i in range(n):\n",
    "        if assigned[i]:\n",
    "            continue\n",
    "        \n",
    "        # Start a new cluster with record i\n",
    "        cluster = [i]\n",
    "        assigned[i] = True\n",
    "        name_parts_i = group.iloc[i]['NAME_PARTS']\n",
    "        \n",
    "        # Find all records that match with record i\n",
    "        for j in range(i + 1, n):\n",
    "            if assigned[j]:\n",
    "                continue\n",
    "            \n",
    "            name_parts_j = group.iloc[j]['NAME_PARTS']\n",
    "            \n",
    "            # Check if at least TWO name components match (fuzzy or substring)\n",
    "            matches = 0\n",
    "            \n",
    "            for part_i in name_parts_i:\n",
    "                for part_j in name_parts_j:\n",
    "                    # Direct fuzzy matching with threshold of 85\n",
    "                    if fuzz.ratio(part_i, part_j) >= 85:\n",
    "                        matches += 1\n",
    "                        break\n",
    "                    # Check if one part is contained in the other (for concatenated names)\n",
    "                    # e.g., \"ANDYBAIDEN\" contains \"ANDY\" or \"BAIDEN\"\n",
    "                    elif len(part_i) >= 3 and len(part_j) >= 3:\n",
    "                        if part_i in part_j or part_j in part_i:\n",
    "                            matches += 1\n",
    "                            break\n",
    "            \n",
    "            # Require at least 2 matching name parts\n",
    "            if matches >= 2:\n",
    "                cluster.append(j)\n",
    "                assigned[j] = True\n",
    "        \n",
    "        clusters.append(cluster)\n",
    "    \n",
    "    # Assign cluster IDs\n",
    "    cluster_ids = [0] * n\n",
    "    for cluster_id, cluster in enumerate(clusters):\n",
    "        for idx in cluster:\n",
    "            cluster_ids[idx] = cluster_id\n",
    "    \n",
    "    return cluster_ids\n",
    "\n",
    "print(\"\\nGrouping by SSNIT and finding name matches (only for valid SSNITs)...\")\n",
    "\n",
    "# Initialize cluster IDs\n",
    "df['CLUSTER_ID'] = -1\n",
    "\n",
    "# Separate valid and invalid SSNIT records\n",
    "valid_ssnit_df = df[~df['INVALID_SSNIT']].copy()\n",
    "invalid_ssnit_df = df[df['INVALID_SSNIT']].copy()\n",
    "\n",
    "print(f\"Valid SSNIT records to process: {len(valid_ssnit_df)}\")\n",
    "print(f\"Invalid SSNIT records (will be kept as-is): {len(invalid_ssnit_df)}\")\n",
    "\n",
    "# Group only valid SSNIT records\n",
    "grouped = valid_ssnit_df.groupby('CLEANED_SSNIT')\n",
    "\n",
    "for ssnit, group in grouped:\n",
    "    if ssnit == '':  # Extra safety check\n",
    "        continue\n",
    "    \n",
    "    indices = group.index\n",
    "    cluster_ids = find_name_matches(group)\n",
    "    df.loc[indices, 'CLUSTER_ID'] = cluster_ids\n",
    "\n",
    "print(f\"Clustering complete!\")\n",
    "\n",
    "# Step 4: Create final grouped dataset\n",
    "# For valid SSNITs: Combine CLEANED_SSNIT and CLUSTER_ID to create unique groups\n",
    "# For invalid SSNITs: Each record stays separate\n",
    "valid_ssnit_df = df[~df['INVALID_SSNIT']].copy()\n",
    "invalid_ssnit_df = df[df['INVALID_SSNIT']].copy()\n",
    "\n",
    "# Process valid SSNIT records\n",
    "if len(valid_ssnit_df) > 0:\n",
    "    valid_ssnit_df['GROUP_KEY'] = valid_ssnit_df['CLEANED_SSNIT'] + '_' + valid_ssnit_df['CLUSTER_ID'].astype(str)\n",
    "    \n",
    "    result_valid = valid_ssnit_df.groupby('GROUP_KEY').agg({\n",
    "        'EMPLOYEE_NUMBER': lambda x: '|'.join(sorted(set(str(v) for v in x))),\n",
    "        'FULL_NAME': 'first',\n",
    "        'CLEANED_SSNIT': 'first',\n",
    "        'SOURCE_FILE': lambda x: '|'.join(sorted(set(str(v) for v in x)))\n",
    "    }).reset_index(drop=True)\n",
    "else:\n",
    "    result_valid = pd.DataFrame(columns=['EMPLOYEE_NUMBER', 'FULL_NAME', 'CLEANED_SSNIT', 'SOURCE_FILE'])\n",
    "\n",
    "# Process invalid SSNIT records - keep them as individual records\n",
    "if len(invalid_ssnit_df) > 0:\n",
    "    result_invalid = invalid_ssnit_df[['EMPLOYEE_NUMBER', 'FULL_NAME', 'CLEANED_SSNIT', 'SOURCE_FILE']].copy()\n",
    "    result_invalid['EMPLOYEE_NUMBER'] = result_invalid['EMPLOYEE_NUMBER'].astype(str)\n",
    "else:\n",
    "    result_invalid = pd.DataFrame(columns=['EMPLOYEE_NUMBER', 'FULL_NAME', 'CLEANED_SSNIT', 'SOURCE_FILE'])\n",
    "\n",
    "# Combine both results\n",
    "result = pd.concat([result_valid, result_invalid], ignore_index=True)\n",
    "\n",
    "# Rename columns\n",
    "result.columns = ['EMPLOYEE_NUMBERS', 'REPRESENTATIVE_NAME', 'SSNIT_NUMBER', 'SOURCE_FILES']\n",
    "\n",
    "# Reorder columns\n",
    "result = result[['SSNIT_NUMBER', 'REPRESENTATIVE_NAME', 'EMPLOYEE_NUMBERS', 'SOURCE_FILES']]\n",
    "\n",
    "print(f\"\\nFinal grouped records: {len(result)}\")\n",
    "print(f\"  - From valid SSNITs (grouped): {len(result_valid)}\")\n",
    "print(f\"  - From invalid SSNITs (kept as-is): {len(result_invalid)}\")\n",
    "print(f\"Original records: {len(df)}\")\n",
    "print(f\"Reduction: {len(df) - len(result)} records consolidated\")\n",
    "\n",
    "# Step 5: Save to Excel\n",
    "output_path = r\"C:\\Users\\spt-admin\\Desktop\\EMPLOYEE_NUM_PER_FIXED.xlsx\"\n",
    "result.to_excel(output_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"\\nSaved to: {output_path}\")\n",
    "\n",
    "# Show sample of results\n",
    "print(\"\\nSample of grouped results:\")\n",
    "print(result.head(10))\n",
    "\n",
    "# Verify the specific SSNIT case\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION: Checking SSNIT 34619361\")\n",
    "print(\"=\"*80)\n",
    "ssnit_34619361 = result[result['SSNIT_NUMBER'] == '34619361']\n",
    "if len(ssnit_34619361) > 0:\n",
    "    print(f\"Found {len(ssnit_34619361)} distinct groups:\")\n",
    "    for idx, row in ssnit_34619361.iterrows():\n",
    "        print(f\"\\nGroup {idx}:\")\n",
    "        print(f\"  SSNIT: {row['SSNIT_NUMBER']}\")\n",
    "        print(f\"  Name: {row['REPRESENTATIVE_NAME']}\")\n",
    "        print(f\"  Employee IDs: {row['EMPLOYEE_NUMBERS']}\")\n",
    "else:\n",
    "    print(\"SSNIT 34619361 not found in results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
