{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6a25d-50d6-46e3-b7b8-832311650a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# File paths\n",
    "input_file = r'C:\\Users\\spt-admin\\Desktop\\NEWD\\EMPLOYER_FIDONE.xlsx'\n",
    "output_file = r'C:\\Users\\spt-admin\\Desktop\\NEWD\\EMPLOYER_SSNIT_CLEAN.xlsx'\n",
    "log_file = r'C:\\Users\\spt-admin\\Desktop\\NEWD\\SSNIT_CONSOLIDATION_LOG.txt'\n",
    "\n",
    "# Read the Excel file\n",
    "print(\"Reading Excel file...\")\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Function to normalize SSNIT (remove trailing zeros)\n",
    "def normalize_ssnit(ssnit):\n",
    "    if pd.isna(ssnit) or str(ssnit).strip() == '':\n",
    "        return None\n",
    "    ssnit_str = str(ssnit).strip()\n",
    "    # Remove decimal point if it exists (from float conversion)\n",
    "    if '.' in ssnit_str:\n",
    "        ssnit_str = ssnit_str.split('.')[0]\n",
    "    # Remove trailing zeros\n",
    "    return ssnit_str.rstrip('0') or '0'\n",
    "\n",
    "# Function to normalize names (remove extra spaces, commas, standardize)\n",
    "def normalize_name(name):\n",
    "    if pd.isna(name):\n",
    "        return ''\n",
    "    # Convert to string and lowercase\n",
    "    name_str = str(name).lower()\n",
    "    # Remove commas\n",
    "    name_str = name_str.replace(',', '')\n",
    "    # Replace multiple spaces with single space\n",
    "    name_str = ' '.join(name_str.split())\n",
    "    # Strip leading/trailing spaces\n",
    "    return name_str.strip()\n",
    "\n",
    "# Add normalized name column\n",
    "df['Name_Normalized'] = df['REPRESENTATIVE_NAME'].apply(normalize_name)\n",
    "\n",
    "# Parse pipe-separated SSNITs and create normalized versions\n",
    "def parse_and_normalize_ssnits(ssnit_string):\n",
    "    if pd.isna(ssnit_string):\n",
    "        return [], []\n",
    "    original = [x.strip() for x in str(ssnit_string).split('|') if x.strip()]\n",
    "    normalized = [normalize_ssnit(s) for s in original]\n",
    "    return original, normalized\n",
    "\n",
    "df['SSNIT_Original_List'] = df['SSNIT_NUMBERS'].apply(lambda x: parse_and_normalize_ssnits(x)[0])\n",
    "df['SSNIT_Normalized_List'] = df['SSNIT_NUMBERS'].apply(lambda x: parse_and_normalize_ssnits(x)[1])\n",
    "\n",
    "# Create log list\n",
    "log_entries = []\n",
    "log_entries.append(f\"=== SSNIT TRAILING ZEROS CONSOLIDATION LOG ===\")\n",
    "log_entries.append(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "log_entries.append(f\"Input file: {input_file}\")\n",
    "log_entries.append(f\"Total records before: {len(df)}\\n\")\n",
    "\n",
    "# Track rows to keep and rows to delete\n",
    "rows_to_delete = []\n",
    "grouped_count = 0\n",
    "\n",
    "# Group by Name_Normalized\n",
    "print(\"Processing duplicates...\")\n",
    "for name, group in df.groupby('Name_Normalized', dropna=False):\n",
    "    if len(group) == 1:\n",
    "        continue  # No duplicates for this name\n",
    "    \n",
    "    # Sort by index to keep nearby rows together\n",
    "    group = group.sort_index()\n",
    "    \n",
    "    # Track which rows have been processed\n",
    "    processed_indices = set()\n",
    "    \n",
    "    # Process each row in the group\n",
    "    for idx in group.index:\n",
    "        if idx in processed_indices:\n",
    "            continue\n",
    "            \n",
    "        current_row = df.loc[idx]\n",
    "        current_ssnits_normalized = current_row['SSNIT_Normalized_List']\n",
    "        \n",
    "        employee_numbers = [str(current_row['EMPLOYEE_NUMBERS'])]\n",
    "        ssnit_numbers = [str(current_row['SSNIT_NUMBERS'])]\n",
    "        merge_indices = [idx]\n",
    "        merge_reasons = []\n",
    "        \n",
    "        # Check for matches with other rows in the same name group\n",
    "        for other_idx in group.index:\n",
    "            if other_idx == idx or other_idx in processed_indices:\n",
    "                continue\n",
    "            \n",
    "            other_row = df.loc[other_idx]\n",
    "            other_ssnits_normalized = other_row['SSNIT_Normalized_List']\n",
    "            \n",
    "            should_merge = False\n",
    "            matched_ssnits = []\n",
    "            \n",
    "            # Check if any normalized SSNIT matches\n",
    "            for curr_ssnit_norm, curr_ssnit_orig in zip(current_ssnits_normalized, current_row['SSNIT_Original_List']):\n",
    "                if curr_ssnit_norm is None:\n",
    "                    continue\n",
    "                for other_ssnit_norm, other_ssnit_orig in zip(other_ssnits_normalized, other_row['SSNIT_Original_List']):\n",
    "                    if other_ssnit_norm is None:\n",
    "                        continue\n",
    "                    if curr_ssnit_norm == other_ssnit_norm and curr_ssnit_orig != other_ssnit_orig:\n",
    "                        # SSNITs match after normalization but are different originally (trailing zeros)\n",
    "                        should_merge = True\n",
    "                        matched_ssnits.append(f\"{curr_ssnit_orig} = {other_ssnit_orig} (normalized to {curr_ssnit_norm})\")\n",
    "            \n",
    "            if should_merge:\n",
    "                employee_numbers.append(str(other_row['EMPLOYEE_NUMBERS']))\n",
    "                ssnit_numbers.append(str(other_row['SSNIT_NUMBERS']))\n",
    "                merge_indices.append(other_idx)\n",
    "                merge_reasons.extend(matched_ssnits)\n",
    "                processed_indices.add(other_idx)\n",
    "        \n",
    "        # If we found duplicates, merge them\n",
    "        if len(employee_numbers) > 1:\n",
    "            grouped_count += 1\n",
    "            \n",
    "            # Combine employee numbers with |\n",
    "            combined_employee_numbers = ' | '.join(employee_numbers)\n",
    "            # Combine SSNIT numbers with |\n",
    "            combined_ssnit_numbers = ' | '.join(ssnit_numbers)\n",
    "            \n",
    "            # Update the first row with combined values\n",
    "            df.at[idx, 'EMPLOYEE_NUMBERS'] = combined_employee_numbers\n",
    "            df.at[idx, 'SSNIT_NUMBERS'] = combined_ssnit_numbers\n",
    "            \n",
    "            # Mark other rows for deletion\n",
    "            rows_to_delete.extend(merge_indices[1:])\n",
    "            \n",
    "            # Log this consolidation\n",
    "            log_entries.append(f\"\\n--- Group {grouped_count} ---\")\n",
    "            log_entries.append(f\"Name: {name}\")\n",
    "            log_entries.append(f\"Combined Employee Numbers: {combined_employee_numbers}\")\n",
    "            log_entries.append(f\"Combined SSNIT Numbers: {combined_ssnit_numbers}\")\n",
    "            log_entries.append(f\"Rows merged: {len(employee_numbers)}\")\n",
    "            for reason in merge_reasons:\n",
    "                log_entries.append(f\"  - {reason}\")\n",
    "        \n",
    "        processed_indices.add(idx)\n",
    "\n",
    "# Remove duplicate rows\n",
    "print(f\"Removing {len(rows_to_delete)} duplicate rows...\")\n",
    "df_cleaned = df.drop(rows_to_delete)\n",
    "\n",
    "# Remove the temporary columns\n",
    "df_cleaned = df_cleaned.drop(['Name_Normalized', 'SSNIT_Original_List', 'SSNIT_Normalized_List'], axis=1)\n",
    "\n",
    "# Save cleaned file\n",
    "print(\"Saving cleaned Excel file...\")\n",
    "df_cleaned.to_excel(output_file, index=False)\n",
    "\n",
    "# Add summary to log\n",
    "log_entries.append(f\"\\n=== SUMMARY ===\")\n",
    "log_entries.append(f\"Total records after: {len(df_cleaned)}\")\n",
    "log_entries.append(f\"Records deleted: {len(rows_to_delete)}\")\n",
    "log_entries.append(f\"Groups consolidated: {grouped_count}\")\n",
    "\n",
    "# Write log file\n",
    "print(\"Writing log file...\")\n",
    "with open(log_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(log_entries))\n",
    "\n",
    "print(f\"\\n Process completed successfully!\")\n",
    "print(f\" Cleaned file saved to: {output_file}\")\n",
    "print(f\" Log file saved to: {log_file}\")\n",
    "print(f\" Records before: {len(df)}\")\n",
    "print(f\" Records after: {len(df_cleaned)}\")\n",
    "print(f\" Records removed: {len(rows_to_delete)}\")\n",
    "print(f\" Groups consolidated: {grouped_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
