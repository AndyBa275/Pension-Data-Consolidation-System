{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722994cf-1b48-4662-8867-f9c1bac6f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# File paths\n",
    "input_file = r'C:\\Users\\spt-admin\\Desktop\\NEWD\\master_list_by_ssnit.xlsx'\n",
    "output_file = r'C:\\Users\\spt-admin\\Desktop\\NEWD\\Master_List_List.xlsx'\n",
    "log_file = r'C:\\Users\\spt-admin\\Desktop\\NEWD_DONE\\CONSOLIDATION_LOG.txt'\n",
    "\n",
    "# Read the Excel file\n",
    "print(\"Reading Excel file...\")\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to normalize names (remove extra spaces, commas, standardize)\n",
    "def normalize_name(name):\n",
    "    if pd.isna(name):\n",
    "        return ''\n",
    "    # Convert to string and lowercase\n",
    "    name_str = str(name).lower()\n",
    "    # Remove commas\n",
    "    name_str = name_str.replace(',', '')\n",
    "    # Replace multiple spaces with single space\n",
    "    name_str = ' '.join(name_str.split())\n",
    "    # Strip leading/trailing spaces\n",
    "    return name_str.strip()\n",
    "\n",
    "# Add normalized name column\n",
    "df['Name_Normalized'] = df['Employee_Name'].apply(normalize_name)\n",
    "\n",
    "# Create log list\n",
    "log_entries = []\n",
    "log_entries.append(f\"=== EMPLOYEE CONSOLIDATION LOG ===\")\n",
    "log_entries.append(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "log_entries.append(f\"Input file: {input_file}\")\n",
    "log_entries.append(f\"Total records before: {len(df)}\\n\")\n",
    "\n",
    "# Function to normalize SSNIT (remove trailing zeros)\n",
    "def normalize_ssnit(ssnit):\n",
    "    if pd.isna(ssnit) or str(ssnit).strip() == '':\n",
    "        return None\n",
    "    ssnit_str = str(ssnit).strip()\n",
    "    # Remove decimal point if it exists (from float conversion)\n",
    "    if '.' in ssnit_str:\n",
    "        ssnit_str = ssnit_str.split('.')[0]\n",
    "    # Remove trailing zeros\n",
    "    return ssnit_str.rstrip('0') or '0'\n",
    "\n",
    "# Add normalized SSNIT column\n",
    "df['SSNIT_Normalized'] = df['SSNIT_Number'].apply(normalize_ssnit)\n",
    "\n",
    "# Track rows to keep and rows to delete\n",
    "rows_to_delete = []\n",
    "grouped_count = 0\n",
    "\n",
    "# Group by Employee_Name\n",
    "print(\"Processing duplicates...\")\n",
    "for name, group in df.groupby('Name_Normalized', dropna=False):\n",
    "    if len(group) == 1:\n",
    "        continue  # No duplicates for this name\n",
    "    \n",
    "    # Sort by index to keep nearby rows together\n",
    "    group = group.sort_index()\n",
    "    \n",
    "    # Track which rows have been processed\n",
    "    processed_indices = set()\n",
    "    \n",
    "    # Process each row in the group\n",
    "    for idx in group.index:\n",
    "        if idx in processed_indices:\n",
    "            continue\n",
    "            \n",
    "        current_row = df.loc[idx]\n",
    "        employee_numbers = [str(current_row['Employee_Number'])]\n",
    "        merge_indices = [idx]\n",
    "        merge_reasons = []\n",
    "        \n",
    "        # Check for matches with other rows in the same name group\n",
    "        for other_idx in group.index:\n",
    "            if other_idx == idx or other_idx in processed_indices:\n",
    "                continue\n",
    "            \n",
    "            other_row = df.loc[other_idx]\n",
    "            should_merge = False\n",
    "            reason = \"\"\n",
    "            \n",
    "            # ONLY CHECK: SSNIT matches after normalization\n",
    "            if (current_row['SSNIT_Normalized'] is not None and \n",
    "                other_row['SSNIT_Normalized'] is not None and\n",
    "                current_row['SSNIT_Normalized'] == other_row['SSNIT_Normalized']):\n",
    "                should_merge = True\n",
    "                reason = f\"SSNIT match: {current_row['SSNIT_Number']} = {other_row['SSNIT_Number']} (normalized to {current_row['SSNIT_Normalized']})\"\n",
    "            \n",
    "            if should_merge:\n",
    "                employee_numbers.append(str(other_row['Employee_Number']))\n",
    "                merge_indices.append(other_idx)\n",
    "                merge_reasons.append(reason)\n",
    "                processed_indices.add(other_idx)\n",
    "        \n",
    "        # If we found duplicates, merge them\n",
    "        if len(employee_numbers) > 1:\n",
    "            grouped_count += 1\n",
    "            \n",
    "            # Combine employee numbers with |\n",
    "            combined_employee_numbers = ' | '.join(employee_numbers)\n",
    "            \n",
    "            # Update the first row with combined employee numbers\n",
    "            df.at[idx, 'Employee_Number'] = combined_employee_numbers\n",
    "            \n",
    "            # Mark other rows for deletion\n",
    "            rows_to_delete.extend(merge_indices[1:])\n",
    "            \n",
    "            # Log this consolidation\n",
    "            log_entries.append(f\"\\n--- Group {grouped_count} ---\")\n",
    "            log_entries.append(f\"Employee Name: {name}\")\n",
    "            log_entries.append(f\"Combined Employee Numbers: {combined_employee_numbers}\")\n",
    "            log_entries.append(f\"Rows merged: {len(employee_numbers)}\")\n",
    "            for i, reason in enumerate(merge_reasons):\n",
    "                log_entries.append(f\"  - {reason}\")\n",
    "            log_entries.append(f\"Original Employee Numbers:\")\n",
    "            for i, emp_num in enumerate(employee_numbers):\n",
    "                orig_idx = merge_indices[i]\n",
    "                orig_row = df.loc[orig_idx]\n",
    "                log_entries.append(f\"  {emp_num}: SSNIT={orig_row['SSNIT_Number']}\")\n",
    "        \n",
    "        processed_indices.add(idx)\n",
    "\n",
    "# Remove duplicate rows\n",
    "print(f\"Removing {len(rows_to_delete)} duplicate rows...\")\n",
    "df_cleaned = df.drop(rows_to_delete)\n",
    "\n",
    "# Remove the temporary normalized columns\n",
    "df_cleaned = df_cleaned.drop(['SSNIT_Normalized', 'Name_Normalized'], axis=1)\n",
    "\n",
    "# Save cleaned file\n",
    "print(\"Saving cleaned Excel file...\")\n",
    "df_cleaned.to_excel(output_file, index=False)\n",
    "\n",
    "# Add summary to log\n",
    "log_entries.append(f\"\\n=== SUMMARY ===\")\n",
    "log_entries.append(f\"Total records after: {len(df_cleaned)}\")\n",
    "log_entries.append(f\"Records deleted: {len(rows_to_delete)}\")\n",
    "log_entries.append(f\"Groups consolidated: {grouped_count}\")\n",
    "\n",
    "# Write log file\n",
    "print(\"Writing log file...\")\n",
    "with open(log_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(log_entries))\n",
    "\n",
    "print(f\"\\n✓ Process completed successfully!\")\n",
    "print(f\"✓ Cleaned file saved to: {output_file}\")\n",
    "print(f\"✓ Log file saved to: {log_file}\")\n",
    "print(f\"✓ Records before: {len(df)}\")\n",
    "print(f\"✓ Records after: {len(df_cleaned)}\")\n",
    "print(f\"✓ Records removed: {len(rows_to_delete)}\")\n",
    "print(f\"✓ Groups consolidated: {grouped_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb286b-6c5f-4a39-aade-9154406a7173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
