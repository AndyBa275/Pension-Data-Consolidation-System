{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9a56e-5e75-46a0-ad86-8cd1b38e1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "SOURCE_FOLDER = r\"C:\\Users\\spt-admin\\Desktop\\PSWEPS_NEW\"\n",
    "OUTPUT_FILE = r\"C:\\Users\\spt-admin\\Desktop\\PSWEPS_NEW\\PSWEPS_MASTER_ANALYSIS.xlsx\"\n",
    "\n",
    "# Required columns (case-insensitive matching)\n",
    "REQUIRED_COLUMNS = ['EMPLOYEE_NUMBER', 'FULL_NAME', 'SSNIT_NUMBER']\n",
    "\n",
    "def log_message(message):\n",
    "    \"\"\"Print message to console\"\"\"\n",
    "    print(message)\n",
    "\n",
    "def has_required_columns(df):\n",
    "    \"\"\"Check if DataFrame has all required columns\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return False\n",
    "    \n",
    "    columns_upper = [str(col).upper().strip() for col in df.columns]\n",
    "    \n",
    "    for req_col in REQUIRED_COLUMNS:\n",
    "        if req_col not in columns_upper:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def standardize_dataframe_columns(df):\n",
    "    \"\"\"Standardize column names to match required columns\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    \n",
    "    # Create mapping of uppercase columns to original columns\n",
    "    col_mapping = {}\n",
    "    for col in df.columns:\n",
    "        col_upper = str(col).upper().strip()\n",
    "        if col_upper in REQUIRED_COLUMNS:\n",
    "            col_mapping[col] = col_upper\n",
    "    \n",
    "    # Rename columns\n",
    "    df_renamed = df.rename(columns=col_mapping)\n",
    "    \n",
    "    return df_renamed\n",
    "\n",
    "def collect_all_data(source_folder):\n",
    "    \"\"\"Collect all employee data from all Excel files\"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    file_count = 0\n",
    "    sheet_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    log_message(\"=\" * 80)\n",
    "    log_message(\"Starting data collection...\")\n",
    "    log_message(\"=\" * 80)\n",
    "    \n",
    "    # Walk through all directories\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        for file in files:\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "            \n",
    "            if ext in ['.xlsx', '.xls', '.xlsm']:\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Skip the output file itself\n",
    "                if file_path == OUTPUT_FILE:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    file_count += 1\n",
    "                    log_message(f\"\\nProcessing: {file}\")\n",
    "                    \n",
    "                    # Get all sheet names\n",
    "                    xl_file = pd.ExcelFile(file_path)\n",
    "                    \n",
    "                    for sheet_name in xl_file.sheet_names:\n",
    "                        try:\n",
    "                            # Read sheet\n",
    "                            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "                            \n",
    "                            # Check if has required columns\n",
    "                            if has_required_columns(df):\n",
    "                                # Standardize column names\n",
    "                                df = standardize_dataframe_columns(df)\n",
    "                                \n",
    "                                # Keep only required columns\n",
    "                                df = df[REQUIRED_COLUMNS]\n",
    "                                \n",
    "                                # Add source tracking columns\n",
    "                                df['SOURCE_FILE'] = file\n",
    "                                df['SOURCE_SHEET'] = sheet_name\n",
    "                                \n",
    "                                # Remove empty rows\n",
    "                                df = df.dropna(how='all', subset=REQUIRED_COLUMNS)\n",
    "                                \n",
    "                                all_data.append(df)\n",
    "                                sheet_count += 1\n",
    "                                log_message(f\"  ✓ Sheet '{sheet_name}': {len(df)} rows collected\")\n",
    "                            else:\n",
    "                                skipped_count += 1\n",
    "                                log_message(f\"  ✗ Sheet '{sheet_name}': Skipped (missing required columns)\")\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            skipped_count += 1\n",
    "                            log_message(f\"  ✗ Sheet '{sheet_name}': Error - {str(e)}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    log_message(f\"  ERROR reading file: {str(e)}\")\n",
    "    \n",
    "    log_message(\"\\n\" + \"=\" * 80)\n",
    "    log_message(f\"Collection Summary:\")\n",
    "    log_message(f\"  Files processed: {file_count}\")\n",
    "    log_message(f\"  Sheets collected: {sheet_count}\")\n",
    "    log_message(f\"  Sheets skipped: {skipped_count}\")\n",
    "    log_message(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    if not all_data:\n",
    "        return None\n",
    "    \n",
    "    # Combine all data\n",
    "    master_df = pd.concat(all_data, ignore_index=True)\n",
    "    return master_df\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normalize name for comparison\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    return str(name).upper().strip().replace(',', '').replace('.', '')\n",
    "\n",
    "def analyze_duplicate_names(master_df):\n",
    "    \"\"\"Find duplicate names using fuzzy matching\"\"\"\n",
    "    log_message(\"Analyzing duplicate names (fuzzy matching)...\")\n",
    "    \n",
    "    # Create normalized names\n",
    "    master_df['NAME_NORMALIZED'] = master_df['FULL_NAME'].apply(normalize_name)\n",
    "    \n",
    "    # Group by normalized name\n",
    "    name_groups = master_df.groupby('NAME_NORMALIZED')\n",
    "    \n",
    "    duplicates = []\n",
    "    \n",
    "    for name, group in name_groups:\n",
    "        if len(group) > 1:\n",
    "            # Check if they have different Employee Numbers or SSNIT\n",
    "            unique_emp = group['EMPLOYEE_NUMBER'].nunique()\n",
    "            unique_ssnit = group['SSNIT_NUMBER'].nunique()\n",
    "            \n",
    "            if unique_emp > 1 or unique_ssnit > 1:\n",
    "                # This is a potential duplicate\n",
    "                for idx, row in group.iterrows():\n",
    "                    duplicates.append({\n",
    "                        'FULL_NAME': row['FULL_NAME'],\n",
    "                        'EMPLOYEE_NUMBER': row['EMPLOYEE_NUMBER'],\n",
    "                        'SSNIT_NUMBER': row['SSNIT_NUMBER'],\n",
    "                        'SOURCE_FILE': row['SOURCE_FILE'],\n",
    "                        'SOURCE_SHEET': row['SOURCE_SHEET'],\n",
    "                        'DUPLICATE_COUNT': len(group),\n",
    "                        'UNIQUE_EMP_NUMBERS': unique_emp,\n",
    "                        'UNIQUE_SSNIT': unique_ssnit\n",
    "                    })\n",
    "    \n",
    "    if duplicates:\n",
    "        dup_df = pd.DataFrame(duplicates)\n",
    "        dup_df = dup_df.sort_values(['FULL_NAME', 'EMPLOYEE_NUMBER'])\n",
    "        return dup_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def analyze_duplicate_employee_numbers(master_df):\n",
    "    \"\"\"Find duplicate Employee Numbers\"\"\"\n",
    "    log_message(\"Analyzing duplicate Employee Numbers...\")\n",
    "    \n",
    "    emp_groups = master_df.groupby('EMPLOYEE_NUMBER')\n",
    "    \n",
    "    duplicates = []\n",
    "    \n",
    "    for emp_num, group in emp_groups:\n",
    "        if len(group) > 1:\n",
    "            # Check if they have different names or SSNIT\n",
    "            unique_names = group['FULL_NAME'].nunique()\n",
    "            unique_ssnit = group['SSNIT_NUMBER'].nunique()\n",
    "            \n",
    "            if unique_names > 1 or unique_ssnit > 1:\n",
    "                for idx, row in group.iterrows():\n",
    "                    duplicates.append({\n",
    "                        'EMPLOYEE_NUMBER': row['EMPLOYEE_NUMBER'],\n",
    "                        'FULL_NAME': row['FULL_NAME'],\n",
    "                        'SSNIT_NUMBER': row['SSNIT_NUMBER'],\n",
    "                        'SOURCE_FILE': row['SOURCE_FILE'],\n",
    "                        'SOURCE_SHEET': row['SOURCE_SHEET'],\n",
    "                        'DUPLICATE_COUNT': len(group),\n",
    "                        'UNIQUE_NAMES': unique_names,\n",
    "                        'UNIQUE_SSNIT': unique_ssnit\n",
    "                    })\n",
    "    \n",
    "    if duplicates:\n",
    "        dup_df = pd.DataFrame(duplicates)\n",
    "        dup_df = dup_df.sort_values(['EMPLOYEE_NUMBER', 'FULL_NAME'])\n",
    "        return dup_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def analyze_duplicate_ssnit(master_df):\n",
    "    \"\"\"Find duplicate SSNIT Numbers\"\"\"\n",
    "    log_message(\"Analyzing duplicate SSNIT Numbers...\")\n",
    "    \n",
    "    # Remove rows with missing SSNIT\n",
    "    df_with_ssnit = master_df[master_df['SSNIT_NUMBER'].notna()]\n",
    "    \n",
    "    ssnit_groups = df_with_ssnit.groupby('SSNIT_NUMBER')\n",
    "    \n",
    "    duplicates = []\n",
    "    \n",
    "    for ssnit, group in ssnit_groups:\n",
    "        if len(group) > 1:\n",
    "            # Check if they have different names or Employee Numbers\n",
    "            unique_names = group['FULL_NAME'].nunique()\n",
    "            unique_emp = group['EMPLOYEE_NUMBER'].nunique()\n",
    "            \n",
    "            if unique_names > 1 or unique_emp > 1:\n",
    "                for idx, row in group.iterrows():\n",
    "                    duplicates.append({\n",
    "                        'SSNIT_NUMBER': row['SSNIT_NUMBER'],\n",
    "                        'EMPLOYEE_NUMBER': row['EMPLOYEE_NUMBER'],\n",
    "                        'FULL_NAME': row['FULL_NAME'],\n",
    "                        'SOURCE_FILE': row['SOURCE_FILE'],\n",
    "                        'SOURCE_SHEET': row['SOURCE_SHEET'],\n",
    "                        'DUPLICATE_COUNT': len(group),\n",
    "                        'UNIQUE_NAMES': unique_names,\n",
    "                        'UNIQUE_EMP_NUMBERS': unique_emp\n",
    "                    })\n",
    "    \n",
    "    if duplicates:\n",
    "        dup_df = pd.DataFrame(duplicates)\n",
    "        dup_df = dup_df.sort_values(['SSNIT_NUMBER', 'FULL_NAME'])\n",
    "        return dup_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def create_summary_statistics(master_df):\n",
    "    \"\"\"Create summary statistics\"\"\"\n",
    "    log_message(\"Creating summary statistics...\")\n",
    "    \n",
    "    stats = []\n",
    "    \n",
    "    # Overall statistics\n",
    "    stats.append({\n",
    "        'METRIC': 'Total Records',\n",
    "        'VALUE': len(master_df)\n",
    "    })\n",
    "    \n",
    "    stats.append({\n",
    "        'METRIC': 'Unique Employee Numbers',\n",
    "        'VALUE': master_df['EMPLOYEE_NUMBER'].nunique()\n",
    "    })\n",
    "    \n",
    "    stats.append({\n",
    "        'METRIC': 'Unique Names',\n",
    "        'VALUE': master_df['FULL_NAME'].nunique()\n",
    "    })\n",
    "    \n",
    "    stats.append({\n",
    "        'METRIC': 'Unique SSNIT Numbers',\n",
    "        'VALUE': master_df['SSNIT_NUMBER'].nunique()\n",
    "    })\n",
    "    \n",
    "    stats.append({\n",
    "        'METRIC': 'Unique Source Files',\n",
    "        'VALUE': master_df['SOURCE_FILE'].nunique()\n",
    "    })\n",
    "    \n",
    "    stats.append({\n",
    "        'METRIC': 'Total Source Sheets',\n",
    "        'VALUE': master_df['SOURCE_SHEET'].nunique()\n",
    "    })\n",
    "    \n",
    "    # Records by source file\n",
    "    file_counts = master_df['SOURCE_FILE'].value_counts().reset_index()\n",
    "    file_counts.columns = ['SOURCE_FILE', 'RECORD_COUNT']\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    \n",
    "    return stats_df, file_counts\n",
    "\n",
    "def analyze_data_quality(master_df):\n",
    "    \"\"\"Analyze data quality issues\"\"\"\n",
    "    log_message(\"Analyzing data quality...\")\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Missing Employee Numbers\n",
    "    missing_emp = master_df[master_df['EMPLOYEE_NUMBER'].isna()]\n",
    "    if len(missing_emp) > 0:\n",
    "        issues.append({\n",
    "            'ISSUE_TYPE': 'Missing Employee Number',\n",
    "            'COUNT': len(missing_emp),\n",
    "            'SAMPLE_FILE': missing_emp.iloc[0]['SOURCE_FILE'] if len(missing_emp) > 0 else ''\n",
    "        })\n",
    "    \n",
    "    # Missing Names\n",
    "    missing_name = master_df[master_df['FULL_NAME'].isna()]\n",
    "    if len(missing_name) > 0:\n",
    "        issues.append({\n",
    "            'ISSUE_TYPE': 'Missing Name',\n",
    "            'COUNT': len(missing_name),\n",
    "            'SAMPLE_FILE': missing_name.iloc[0]['SOURCE_FILE'] if len(missing_name) > 0 else ''\n",
    "        })\n",
    "    \n",
    "    # Missing SSNIT\n",
    "    missing_ssnit = master_df[master_df['SSNIT_NUMBER'].isna()]\n",
    "    if len(missing_ssnit) > 0:\n",
    "        issues.append({\n",
    "            'ISSUE_TYPE': 'Missing SSNIT Number',\n",
    "            'COUNT': len(missing_ssnit),\n",
    "            'SAMPLE_FILE': missing_ssnit.iloc[0]['SOURCE_FILE'] if len(missing_ssnit) > 0 else ''\n",
    "        })\n",
    "    \n",
    "    # Short names (potential data quality issue)\n",
    "    short_names = master_df[master_df['FULL_NAME'].str.len() < 3]\n",
    "    if len(short_names) > 0:\n",
    "        issues.append({\n",
    "            'ISSUE_TYPE': 'Very Short Name (< 3 chars)',\n",
    "            'COUNT': len(short_names),\n",
    "            'SAMPLE_FILE': short_names.iloc[0]['SOURCE_FILE'] if len(short_names) > 0 else ''\n",
    "        })\n",
    "    \n",
    "    if issues:\n",
    "        return pd.DataFrame(issues)\n",
    "    else:\n",
    "        return pd.DataFrame({'ISSUE_TYPE': ['No Issues Found'], 'COUNT': [0], 'SAMPLE_FILE': ['']})\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    log_message(\"\\n\" + \"=\" * 80)\n",
    "    log_message(\"MASTER CONSOLIDATION & ANALYSIS\")\n",
    "    log_message(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Step 1: Collect all data\n",
    "    master_df = collect_all_data(SOURCE_FOLDER)\n",
    "    \n",
    "    if master_df is None or master_df.empty:\n",
    "        log_message(\"ERROR: No data collected. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    log_message(f\"Total records collected: {len(master_df)}\\n\")\n",
    "    \n",
    "    # Step 2: Perform analyses\n",
    "    log_message(\"Performing analyses...\")\n",
    "    \n",
    "    dup_names_df = analyze_duplicate_names(master_df)\n",
    "    dup_emp_df = analyze_duplicate_employee_numbers(master_df)\n",
    "    dup_ssnit_df = analyze_duplicate_ssnit(master_df)\n",
    "    stats_df, file_counts_df = create_summary_statistics(master_df)\n",
    "    quality_df = analyze_data_quality(master_df)\n",
    "    \n",
    "    # Step 3: Write to Excel\n",
    "    log_message(f\"\\nWriting results to: {OUTPUT_FILE}\")\n",
    "    \n",
    "    with pd.ExcelWriter(OUTPUT_FILE, engine='openpyxl') as writer:\n",
    "        # Sheet 1: Master Data\n",
    "        master_df.to_excel(writer, sheet_name='Master Data', index=False)\n",
    "        \n",
    "        # Sheet 2: Duplicate Names\n",
    "        if not dup_names_df.empty:\n",
    "            dup_names_df.to_excel(writer, sheet_name='Duplicate Names', index=False)\n",
    "        else:\n",
    "            pd.DataFrame({'MESSAGE': ['No duplicate names found']}).to_excel(writer, sheet_name='Duplicate Names', index=False)\n",
    "        \n",
    "        # Sheet 3: Duplicate Employee Numbers\n",
    "        if not dup_emp_df.empty:\n",
    "            dup_emp_df.to_excel(writer, sheet_name='Duplicate Employee Numbers', index=False)\n",
    "        else:\n",
    "            pd.DataFrame({'MESSAGE': ['No duplicate employee numbers found']}).to_excel(writer, sheet_name='Duplicate Employee Numbers', index=False)\n",
    "        \n",
    "        # Sheet 4: Duplicate SSNIT\n",
    "        if not dup_ssnit_df.empty:\n",
    "            dup_ssnit_df.to_excel(writer, sheet_name='Duplicate SSNIT', index=False)\n",
    "        else:\n",
    "            pd.DataFrame({'MESSAGE': ['No duplicate SSNIT numbers found']}).to_excel(writer, sheet_name='Duplicate SSNIT', index=False)\n",
    "        \n",
    "        # Sheet 5: Summary Statistics\n",
    "        stats_df.to_excel(writer, sheet_name='Summary Statistics', index=False)\n",
    "        \n",
    "        # Add file counts\n",
    "        file_counts_df.to_excel(writer, sheet_name='Records by File', index=False)\n",
    "        \n",
    "        # Sheet 6: Data Quality Issues\n",
    "        quality_df.to_excel(writer, sheet_name='Data Quality Issues', index=False)\n",
    "    \n",
    "    log_message(\" Excel file created successfully!\")\n",
    "    \n",
    "    # Final summary\n",
    "    log_message(\"\\n\" + \"=\" * 80)\n",
    "    log_message(\"ANALYSIS COMPLETE\")\n",
    "    log_message(\"=\" * 80)\n",
    "    log_message(f\"Master records: {len(master_df)}\")\n",
    "    log_message(f\"Duplicate names found: {len(dup_names_df)}\")\n",
    "    log_message(f\"Duplicate employee numbers: {len(dup_emp_df)}\")\n",
    "    log_message(f\"Duplicate SSNIT numbers: {len(dup_ssnit_df)}\")\n",
    "    log_message(f\"\\nOutput file: {OUTPUT_FILE}\")\n",
    "    log_message(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
